services:
  # LiveKit Server - Voice/video infrastructure
  livekit:
    image: livekit/livekit-server:latest
    command: --config /etc/livekit.yaml --dev
    ports:
      - "${LIVEKIT_HTTP_PORT:-7880}:${LIVEKIT_HTTP_PORT:-7880}" # HTTP/WebSocket
      - "${LIVEKIT_RTC_TCP_PORT:-7881}:${LIVEKIT_RTC_TCP_PORT:-7881}" # RTC over TCP
      - "${LIVEKIT_TCP_FALLBACK_PORT:-7882}:${LIVEKIT_TCP_FALLBACK_PORT:-7882}" # TCP fallback
      - "${LIVEKIT_UDP_PORT_START:-40000}-${LIVEKIT_UDP_PORT_END:-40100}:${LIVEKIT_UDP_PORT_START:-40000}-${LIVEKIT_UDP_PORT_END:-40100}/udp" # WebRTC UDP
    volumes:
      - ./livekit-config.yaml:/etc/livekit.yaml:ro
    depends_on:
      redis:
        condition: service_healthy
    networks:
      - foundry
    healthcheck:
      test: ["CMD", "wget", "--spider", "-q", "http://localhost:7880/"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 15s

  # Redis - State management for LangGraph + LiveKit
  redis:
    image: redis:7-alpine
    ports:
      - "${REDIS_PORT:-6379}:${REDIS_PORT:-6379}"
    volumes:
      - redis-data:/data
    networks:
      - foundry
    command: redis-server --appendonly yes --port ${REDIS_PORT:-6379}
    healthcheck:
      test: ["CMD", "redis-cli", "-p", "${REDIS_PORT:-6379}", "ping"]
      interval: 10s
      timeout: 5s
      retries: 5

  # Agent Foundry UI - Next.js Frontend with LiveKit integration
  agent-foundry-ui:
    build:
      context: .
      dockerfile: Dockerfile.frontend
    ports:
      - "${FRONTEND_PORT:-3000}:3000" # External access only
    volumes:
      # Mount source for hot reload (everything except node_modules)
      - ./app:/app/app
      - ./public:/app/public
      - ./styles:/app/styles
      - ./lib:/app/lib
      - ./components:/app/components
      - ./hooks:/app/hooks
      - ./package.json:/app/package.json
      - ./tsconfig.json:/app/tsconfig.json
      - ./next.config.js:/app/next.config.js
      - ./tailwind.config.ts:/app/tailwind.config.ts
      - ./postcss.config.js:/app/postcss.config.js
      # Prevent overwriting node_modules
      - /app/node_modules
      - /app/.next
    environment:
      # ========================================================================
      # PUBLIC URLS - For browser/client access (localhost in dev)
      # ========================================================================
      - NEXT_PUBLIC_API_URL=http://localhost:${BACKEND_PORT:-8000}
      - NEXT_PUBLIC_WS_URL=ws://localhost:${BACKEND_PORT:-8000}
      - NEXT_PUBLIC_LIVEKIT_URL=ws://localhost:${LIVEKIT_HTTP_PORT:-7880}

      # ========================================================================
      # INTERNAL SERVICE DISCOVERY - Server-side Next.js API routes
      # Uses DNS service names, no port variables needed
      # ========================================================================
      - BACKEND_URL=http://foundry-backend:8080
      - SVC_COMPILER_URL=http://foundry-compiler:8080

      # ========================================================================
      # API KEYS - Server-side only
      # ========================================================================
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      - DEEPGRAM_API_KEY=${DEEPGRAM_API_KEY}
      - ANTHROPIC_API_KEY=${ANTHROPIC_API_KEY}

      # Feature flags
      - NEXT_PUBLIC_ENABLE_WEBSOCKET=true

      # ========================================================================
      # AUTH CONFIG - Loaded from service registry (LocalStack)
      # ZITADEL_CLIENT_ID and ZITADEL_URL are fetched from /api/config/auth
      # Only NEXTAUTH_SECRET needs to be set (for JWT signing)
      # ========================================================================
      - NEXTAUTH_URL=http://localhost:3000
      - NEXTAUTH_SECRET=${NEXTAUTH_SECRET:-change-me-in-production}
    depends_on:
      foundry-backend:
        condition: service_healthy
      livekit:
        condition: service_started
      zitadel-proxy:
        condition: service_started
    networks:
      - foundry

  # Foundry Backend - FastAPI MCP Server + LangGraph agents + LiveKit integration
  foundry-backend:
    build:
      context: .
      dockerfile: backend/Dockerfile
    command: bash backend/startup.sh
    ports:
      # External port mapping for localhost development
      - "${BACKEND_PORT:-8000}:8080" # Maps internal 8080 to external 8000
    volumes:
      - ./backend:/app/backend
      - ./agent:/app/agent
      - ./agents:/app/agents
      - ./data:/app/data
      - ./mcp:/app/mcp
      - ./scripts:/app/scripts
      - ./openfga:/app/openfga
      # Mount local n8n source for mining integrations (read-only)
      - /Users/nwalker/Development/Projects/n8n:/n8n:ro
    environment:
      # ========================================================================
      # SERVICE LISTENING - Always 8080 internally
      # ========================================================================
      - PORT=8080

      # ========================================================================
      # SERVICE DISCOVERY - DNS-based (Infrastructure as Registry)
      # All internal services use standardized ports, no variables needed
      # ========================================================================
      - SVC_LIVEKIT_HOST=livekit
      - SVC_REDIS_HOST=redis
      - SVC_POSTGRES_HOST=postgres
      - SVC_MCP_HOST=mcp-integration
      - SVC_N8N_HOST=n8n

      # ========================================================================
      # INFRASTRUCTURE SERVICE PORTS (non-standard ports)
      # ========================================================================
      - LIVEKIT_HTTP_PORT=7880
      - REDIS_PORT=6379
      - POSTGRES_PORT=5432

      # ========================================================================
      # LIVEKIT CREDENTIALS - Required for voice session management
      # ========================================================================
      - LIVEKIT_API_KEY=${LIVEKIT_API_KEY}
      - LIVEKIT_API_SECRET=${LIVEKIT_API_SECRET}

      # ========================================================================
      # PUBLIC URLS - For responses to frontend
      # ========================================================================
      - LIVEKIT_PUBLIC_URL=${LIVEKIT_PUBLIC_URL:-ws://localhost:7880}

      # ========================================================================
      # DATABASE & STORAGE
      # PostgreSQL with Row-Level Security (RLS) for tenant isolation
      # ========================================================================
      - DATABASE_URL=postgresql://foundry:foundry@postgres:5432/foundry
      - POSTGRES_ADMIN_URL=postgresql://foundry:foundry@postgres:5432/foundry
      - REDIS_ADMIN_URL=redis://redis:6379
      - MONGODB_ADMIN_URL=mongodb://foundry:foundry@mongodb:27017/?authSource=admin
      - MONGODB_ADMIN_DATABASE=agentfoundry

      # ========================================================================
      # INTEGRATION CREDENTIALS
      # ========================================================================
      - N8N_API_KEY=${N8N_API_KEY}
      - N8N_BASIC_AUTH_USER=${N8N_BASIC_AUTH_USER:-admin}
      - N8N_BASIC_AUTH_PASSWORD=${N8N_BASIC_AUTH_PASSWORD:-foundry2024}

      # ========================================================================
      # LOCALSTACK / SECRETS MANAGEMENT
      # Uses service discovery - no endpoint URL needed
      # ========================================================================
      - SVC_LOCALSTACK_HOST=localstack
      - LOCALSTACK_PORT=4566
      - AWS_REGION=us-east-1

      # ========================================================================
      # OPENFGA / AUTHORIZATION
      # Uses service discovery pattern
      # Configuration (store_id, model_id) retrieved from LocalStack, not .env!
      # ========================================================================
      - SVC_OPENFGA_HOST=openfga
      - OPENFGA_HTTP_PORT=8081
      - OPENFGA_GRPC_PORT=8080

      # ========================================================================
      # ZITADEL / IDENTITY & ACCESS MANAGEMENT
      # Uses service discovery pattern
      # Configuration (client_id, client_secret) retrieved from LocalStack
      # ========================================================================
      - SVC_ZITADEL_HOST=zitadel
      - ZITADEL_HTTP_PORT=8080
      # Note: OPENFGA_STORE_ID and OPENFGA_AUTH_MODEL_ID are stored in LocalStack
      # Run: python scripts/openfga_setup_localstack.py <store_id> <model_id>

      # ========================================================================
      # TEMPO / DISTRIBUTED TRACING
      # Uses service discovery pattern - sends OTLP traces to Tempo
      # ========================================================================
      - SVC_TEMPO_HOST=tempo
      - OTEL_EXPORTER_OTLP_ENDPOINT=http://tempo:4318
      - OTEL_SERVICE_NAME=foundry-backend
      - OTEL_TRACES_EXPORTER=otlp
      - OTEL_METRICS_EXPORTER=prometheus

      # ========================================================================
      # MAILPIT / EMAIL TESTING
      # Uses service discovery pattern - SMTP config stored in LocalStack
      # ========================================================================
      - SVC_MAILPIT_HOST=mailpit
      - MAILPIT_SMTP_PORT=1025

      # ========================================================================
      # API KEYS (Legacy - Will be migrated to LocalStack Secrets Manager)
      # ========================================================================
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      - ANTHROPIC_API_KEY=${ANTHROPIC_API_KEY}
      - DEEPGRAM_API_KEY=${DEEPGRAM_API_KEY}
      - ELEVENLABS_API_KEY=${ELEVENLABS_API_KEY}
      - NOTION_API_TOKEN=${NOTION_API_TOKEN}
      - NOTION_DATABASE_STORIES_ID=${NOTION_DATABASE_STORIES_ID}
      - NOTION_DATABASE_EPICS_ID=${NOTION_DATABASE_EPICS_ID}
      - GITHUB_TOKEN=${GITHUB_TOKEN}
      - GITHUB_REPO=${GITHUB_REPO}
      - GITHUB_DEFAULT_BRANCH=${GITHUB_DEFAULT_BRANCH}
    depends_on:
      redis:
        condition: service_healthy
      livekit:
        condition: service_started
      n8n:
        condition: service_started
      mcp-integration:
        condition: service_healthy
      localstack:
        condition: service_healthy
      openfga:
        condition: service_started
    healthcheck:
      test: ["CMD", "curl", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
    networks:
      - foundry

  # Foundry Compiler - DIS Parser + Agent YAML Generator
  foundry-compiler:
    build:
      context: .
      dockerfile: compiler/Dockerfile
    ports:
      - "${COMPILER_PORT:-8002}:8080" # External 8002 maps to internal 8080
    volumes:
      - ./compiler:/app/compiler
      - ./agents:/app/agents
      - ./data:/app/data
    environment:
      # Standardized internal port
      - PORT=8080
      # Service discovery
      - SVC_POSTGRES_HOST=postgres
      # Database connection
      - DATABASE_URL=postgresql://foundry:foundry@postgres:5432/foundry
      - DIS_SCHEMA_URL=https://schemas.domainintelligenceschema.org/dis/1.6.0
    networks:
      - foundry

  # Forge Service - Agent Configuration and Management API
  # NOTE: Commented out - forge_service/ directory not yet implemented
  # forge-service:
  #   build:
  #     context: .
  #     dockerfile: forge_service/Dockerfile
  #   ports:
  #     - "${FORGE_PORT:-8003}:${FORGE_PORT:-8003}"
  #   volumes:
  #     - ./forge_service:/app/forge_service
  #     - ./agents:/app/agents
  #     - ./data:/app/data
  #   environment:
  #     - PORT=${FORGE_PORT:-8003}
  #     - DATABASE_URL=postgresql://foundry:foundry@postgres:5432/foundry
  #     - AGENTS_DIR=/app/agents
  #     - DATA_DIR=/app/data
  #   networks:
  #     - foundry

  # Voice Agent Worker - LiveKit agent that processes audio through LangGraph
  voice-agent-worker:
    build:
      context: .
      dockerfile: backend/Dockerfile
    command: python backend/io_agent_worker.py start # Supervisor routing with streaming TTS
    volumes:
      - ./backend:/app/backend
      - ./agent:/app/agent
      - ./agents:/app/agents
      - ./data:/app/data
    environment:
      # ========================================================================
      # LIVEKIT CONNECTION - SDK requires LIVEKIT_URL env var
      # ========================================================================
      - LIVEKIT_URL=ws://livekit:7880
      - LIVEKIT_API_KEY=${LIVEKIT_API_KEY}
      - LIVEKIT_API_SECRET=${LIVEKIT_API_SECRET}

      # ========================================================================
      # SERVICE DISCOVERY - DNS-based
      # ========================================================================
      - SVC_LIVEKIT_HOST=livekit
      - SVC_BACKEND_HOST=foundry-backend
      - LIVEKIT_HTTP_PORT=7880

      # ========================================================================
      # API KEYS - STT/TTS
      # ========================================================================
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      - DEEPGRAM_API_KEY=${DEEPGRAM_API_KEY}
      - ELEVEN_API_KEY=${ELEVENLABS_API_KEY}
    depends_on:
      livekit:
        condition: service_started
      foundry-backend:
        condition: service_healthy
    networks:
      - foundry
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "python", "-c", "import sys; sys.exit(0)"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s

  # n8n - Workflow automation platform
  n8n:
    image: n8nio/n8n:latest
    restart: unless-stopped
    environment:
      # Basic config
      - N8N_HOST=n8n
      - N8N_PORT=5678
      - N8N_PROTOCOL=http
      - NODE_ENV=production

      # Disable security for local dev (Docker network is already isolated)
      - N8N_SECURE_COOKIE=false
      - N8N_DIAGNOSTICS_ENABLED=false
      - N8N_PERSONALIZATION_ENABLED=false
      - N8N_PUBLIC_API_DISABLED=false
      - N8N_BASIC_AUTH_ACTIVE=false
      - N8N_USER_MANAGEMENT_DISABLED=true

      # For encryption (sessions, creds) â€“ set and keep stable
      - N8N_ENCRYPTION_KEY=${N8N_ENCRYPTION_KEY:-super-secret-long-key}

      # Use built-in sqlite + volume for now (simple, fast)
      - N8N_SAVE_DATA_MODE=filesystem
      - N8N_SAVE_CREDENTIALS=filesystem
      - N8N_PATH=/ # base path

    # For local dev, expose port; for prod, you can omit ports so it stays internal only
    ports:
      - "5678:5678"

    volumes:
      - n8n-data:/home/node/.n8n

    networks:
      - foundry

  # MCP Integration Gateway - Bridges MCP tool calls to n8n workflows
  mcp-integration:
    build:
      context: .
      dockerfile: mcp/integration_server/Dockerfile
    ports:
      - "${MCP_INTEGRATION_PORT:-8100}:8080" # External 8100 maps to internal 8080
    volumes:
      - ./mcp:/app/mcp
    environment:
      # Standardized internal port
      - PORT=8080
      # Service discovery
      - SVC_N8N_HOST=n8n
      - N8N_PORT=5678
      # Configuration
      - INTEGRATIONS_MANIFEST=/app/mcp/integration_server/config/integrations.manifest.json
    depends_on:
      n8n:
        condition: service_started
    networks:
      - foundry
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8100/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s
    restart: unless-stopped

  # Official MCP Server - Model Context Protocol with @modelcontextprotocol/sdk
  # NOTE: Commented out temporarily while SDK API is being updated
  # See mcp-server/NEXT_STEPS.md for completion guide
  #
  # mcp-server:
  #   build:
  #     context: ./mcp-server
  #     dockerfile: Dockerfile
  #   ports:
  #     - "${MCP_SERVER_PORT:-3100}:3100"
  #   volumes:
  #     - ./mcp-server/src:/app/src
  #   environment:
  #     - SVC_BACKEND_HOST=foundry-backend
  #     - SVC_MCP_HOST=mcp-integration
  #     - BACKEND_PORT=8080
  #     - MCP_INTEGRATION_PORT=8080
  #     - ENVIRONMENT=development
  #   depends_on:
  #     foundry-backend:
  #       condition: service_healthy
  #     mcp-integration:
  #       condition: service_started
  #   networks:
  #     - foundry
  #   command: ["npm", "run", "dev"]

  # Database - PostgreSQL for control plane (orgs/domains/instances/users)
  postgres:
    image: postgres:16-alpine
    environment:
      - POSTGRES_DB=foundry
      - POSTGRES_USER=foundry
      - POSTGRES_PASSWORD=foundry
    ports:
      - "5432:5432"
    volumes:
      - postgres-data:/var/lib/postgresql/data
    networks:
      - foundry
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U foundry -d foundry"]
      interval: 10s
      timeout: 5s
      retries: 5

  # LocalStack - AWS Services Emulation (Secrets Manager, S3, etc.)
  localstack:
    image: localstack/localstack:latest
    ports:
      - "4566:4566" # LocalStack gateway
      - "4510-4559:4510-4559" # External services port range
    environment:
      # Services to enable
      - SERVICES=secretsmanager,s3,dynamodb
      # Configuration
      - DEBUG=${DEBUG-0}
      - DOCKER_HOST=unix:///var/run/docker.sock
      - LOCALSTACK_HOST=localstack
      # Persistence
      - PERSISTENCE=1
      - DATA_DIR=/var/lib/localstack/data
    volumes:
      - localstack-data:/var/lib/localstack
      - /var/run/docker.sock:/var/run/docker.sock
    networks:
      - foundry
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:4566/_localstack/health"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 10s

  # OpenFGA - Fine-Grained Authorization (Zanzibar)
  openfga:
    image: openfga/openfga:latest
    command: run
    ports:
      - "${OPENFGA_HTTP_PORT:-9080}:8080" # HTTP API (internal 8080)
      - "${OPENFGA_GRPC_PORT:-8081}:8081" # gRPC
    environment:
      # Use PostgreSQL for storage
      - OPENFGA_DATASTORE_ENGINE=postgres
      - OPENFGA_DATASTORE_URI=postgres://foundry:foundry@postgres:5432/foundry?sslmode=disable
      # Logging
      - OPENFGA_LOG_FORMAT=json
      - OPENFGA_LOG_LEVEL=info
      # Performance
      - OPENFGA_MAX_TUPLES_PER_WRITE=100
      - OPENFGA_MAX_TYPES_PER_MODEL=100
      - OPENFGA_CHANGELOGCONSUMPTION_HORIZON_OFFSET=0
    depends_on:
      postgres:
        condition: service_healthy
    networks:
      - foundry
    healthcheck:
      test: ["CMD", "wget", "--spider", "-q", "http://localhost:8080/healthz"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 10s
    restart: unless-stopped

  # Zitadel - Identity & Access Management (OIDC/OAuth2)
  zitadel:
    image: ghcr.io/zitadel/zitadel:v2.62.1
    command: start-from-init --masterkeyFromEnv --tlsMode disabled
    ports:
      - "${ZITADEL_HTTP_PORT:-8082}:8080"  # HTTP API
    environment:
      # Database - Use PostgreSQL (shared foundry database, separate schema)
      - ZITADEL_DATABASE_POSTGRES_HOST=postgres
      - ZITADEL_DATABASE_POSTGRES_PORT=5432
      - ZITADEL_DATABASE_POSTGRES_DATABASE=foundry
      - ZITADEL_DATABASE_POSTGRES_USER_USERNAME=foundry
      - ZITADEL_DATABASE_POSTGRES_USER_PASSWORD=foundry
      - ZITADEL_DATABASE_POSTGRES_USER_SSL_MODE=disable
      - ZITADEL_DATABASE_POSTGRES_ADMIN_USERNAME=foundry
      - ZITADEL_DATABASE_POSTGRES_ADMIN_PASSWORD=foundry
      - ZITADEL_DATABASE_POSTGRES_ADMIN_SSL_MODE=disable
      # Master key for encryption (32 bytes base64)
      - ZITADEL_MASTERKEY=MasterkeyNeedsToHave32Characters
      # External domain configuration (for local dev)
      - ZITADEL_EXTERNALDOMAIN=localhost
      - ZITADEL_EXTERNALPORT=8082
      - ZITADEL_EXTERNALSECURE=false
      # First instance configuration
      - ZITADEL_FIRSTINSTANCE_ORG_NAME=AgentFoundry
      - ZITADEL_FIRSTINSTANCE_ORG_HUMAN_USERNAME=admin
      - ZITADEL_FIRSTINSTANCE_ORG_HUMAN_PASSWORD=AgentFoundry2024!
      # Enable Login V2 UI
      - ZITADEL_DEFAULTINSTANCE_LOGINV2_ENABLED=true
      - ZITADEL_SYSTEMDEFAULTS_LOGINPOLICY_LOGINV2=true
      # Logging
      - ZITADEL_LOG_LEVEL=debug
      # SMTP Configuration - Configure manually in Zitadel UI after startup
      # Host: mailpit (or smtp-proxy), Port: 1025, No TLS, Any username/password
      # See: http://localhost:8082 -> Instance Settings -> SMTP
    depends_on:
      postgres:
        condition: service_healthy
    extra_hosts:
      - "host.docker.internal:host-gateway"
      - "smtp-proxy:172.18.0.200"
      - "mailpit:172.18.0.201"
    networks:
      - foundry
    healthcheck:
      # Zitadel uses distroless image - disable healthcheck as ready command expects HTTPS
      # Zitadel is verified working via curl http://localhost:8082/debug/healthz from host
      disable: true
    restart: unless-stopped

  # Zitadel Proxy - Nginx reverse proxy that adds correct Host header
  # Required because Zitadel uses virtual hosting and only accepts "localhost:8082"
  # Docker containers can't reach host's localhost, so this proxy translates
  zitadel-proxy:
    image: nginx:alpine
    volumes:
      - ./nginx/zitadel-proxy.conf:/etc/nginx/conf.d/default.conf:ro
    depends_on:
      zitadel:
        condition: service_started
    networks:
      - foundry
    healthcheck:
      test: ["CMD", "wget", "--spider", "-q", "http://localhost:8080/"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 10s
    restart: unless-stopped

  # ============================================================================
  # EMAIL TESTING
  # ============================================================================

  # Mailpit - Local email testing (captures all outbound emails)
  mailpit:
    image: axllent/mailpit:latest
    ports:
      - "${MAILPIT_SMTP_PORT:-1025}:1025"  # SMTP server
      - "${MAILPIT_UI_PORT:-8025}:8025"    # Web UI
    environment:
      - MP_MAX_MESSAGES=500
      - MP_SMTP_AUTH_ACCEPT_ANY=true
      - MP_SMTP_AUTH_ALLOW_INSECURE=true
    networks:
      foundry:
        ipv4_address: 172.18.0.201
    restart: unless-stopped

  # SMTP Proxy - TCP proxy for services that can't resolve internal DNS (e.g., Zitadel distroless)
  smtp-proxy:
    image: alpine/socat:latest
    command: TCP-LISTEN:1025,fork,reuseaddr TCP:172.18.0.201:1025
    depends_on:
      - mailpit
    networks:
      foundry:
        ipv4_address: 172.18.0.200
    restart: unless-stopped

  # ============================================================================
  # OBSERVABILITY STACK (Phase 2)
  # ============================================================================

  # Prometheus - Metrics collection and storage
  prometheus:
    image: prom/prometheus:v2.47.0
    ports:
      - "${PROMETHEUS_PORT:-9090}:9090"
    volumes:
      - ./observability/prometheus.yml:/etc/prometheus/prometheus.yml:ro
      - prometheus-data:/prometheus
    command:
      - "--config.file=/etc/prometheus/prometheus.yml"
      - "--storage.tsdb.path=/prometheus"
      - "--web.enable-lifecycle"
      - "--storage.tsdb.retention.time=15d"
    networks:
      - foundry
    restart: unless-stopped

  # Grafana - Metrics visualization and dashboards
  grafana:
    image: grafana/grafana:10.2.0
    ports:
      - "${GRAFANA_PORT:-3001}:3000"
    volumes:
      - grafana-data:/var/lib/grafana
      - ./observability/grafana/provisioning:/etc/grafana/provisioning:ro
    environment:
      - GF_SECURITY_ADMIN_USER=${GRAFANA_ADMIN_USER:-admin}
      - GF_SECURITY_ADMIN_PASSWORD=${GRAFANA_ADMIN_PASSWORD:-foundry}
      - GF_USERS_ALLOW_SIGN_UP=false
      - GF_SERVER_ROOT_URL=http://localhost:${GRAFANA_PORT:-3001}
    depends_on:
      - prometheus
    networks:
      - foundry
    restart: unless-stopped

  # Tempo - Distributed Tracing Backend (receives OTLP traces)
  tempo:
    image: grafana/tempo:latest
    command: ["-config.file=/etc/tempo.yaml"]
    ports:
      - "${TEMPO_OTLP_GRPC_PORT:-4317}:4317" # OTLP gRPC
      - "${TEMPO_OTLP_HTTP_PORT:-4318}:4318" # OTLP HTTP
      - "${TEMPO_QUERY_PORT:-3200}:3200" # Tempo query API (for Grafana)
    volumes:
      - ./observability/tempo.yaml:/etc/tempo.yaml:ro
      - tempo-data:/var/tempo
    networks:
      - foundry
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost:3200/ready"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 10s

  # OpenTelemetry Collector (optional - for complex routing)
  # Uncomment if you need advanced trace processing/routing
  # otel-collector:
  #   image: otel/opentelemetry-collector-contrib:0.88.0
  #   ports:
  #     - "14317:4317"  # OTLP gRPC (different port to avoid conflict with Tempo)
  #     - "14318:4318"  # OTLP HTTP
  #     - "8889:8889"   # Prometheus metrics
  #   volumes:
  #     - ./observability/otel-collector.yml:/etc/otelcol/config.yaml:ro
  #   command: ["--config", "/etc/otelcol/config.yaml"]
  #   networks:
  #     - foundry
  #   restart: unless-stopped

networks:
  foundry:
    driver: bridge
    ipam:
      config:
        - subnet: 172.18.0.0/16

volumes:
  redis-data:
  n8n-data:
  postgres-data:
  prometheus-data:
  grafana-data:
  localstack-data:
  openfga-data:
  tempo-data:
  zitadel-data:
