"""
ReactFlow to Python LangGraph Converter
Backend implementation for generating Python LangGraph code from ReactFlow graphs
"""

import re
from typing import Any


def reactflow_to_python(graph_data: dict[str, Any]) -> str:
    """
    Convert ReactFlow graph data to Python LangGraph code.

    Args:
        graph_data: Dictionary containing:
            - nodes: List of node objects
            - edges: List of edge objects
            - agentName: Name of the agent

    Returns:
        str: Generated Python LangGraph code
    """
    nodes = graph_data.get("nodes", [])
    edges = graph_data.get("edges", [])
    agent_name = graph_data.get("agentName", "untitled_agent")

    if not nodes:
        return "# No nodes to generate code from\n"

    # Find entry point
    entry_node = next((n for n in nodes if n.get("type") == "entryPoint"), None)
    if not entry_node:
        return "# Error: No entry point node found\n"

    code_lines = []

    # Header
    code_lines.extend(
        [
            '"""',
            f"{agent_name} - LangGraph Agent",
            "Auto-generated from Agent Foundry visual designer",
            "Generated by: Agent Foundry Backend Converter",
            '"""',
            "",
        ]
    )

    # Imports
    code_lines.extend(
        [
            "from typing import Dict, Any",
            "from langchain_core.messages import BaseMessage, HumanMessage, AIMessage, SystemMessage",
            "from langchain_openai import ChatOpenAI",
            "from langchain_anthropic import ChatAnthropic",
            "from langgraph.types import interrupt, Command",
            "",
            "# Import global AgentState from shared state module",
            "import sys",
            'sys.path.insert(0, "/app")',
            "from agents.state import AgentState",
            "",
        ]
    )

    # Agent class with node implementations
    # Generate class name: sanitize, title case, remove all special chars
    class_name = f"{agent_name.replace('-', '_').replace(' ', '_').title().replace('_', '')}Agent"

    # Python class names cannot start with a digit - prefix with "Agent" if needed
    if class_name[0].isdigit():
        class_name = f"Agent{class_name}"

    code_lines.extend(
        [
            "# ============================================",
            "# Agent Class",
            "# ============================================",
            "",
            f"class {class_name}:",
            '    """Agent class containing all node handlers"""',
            "    ",
        ]
    )

    # Generate method for each node
    for node in nodes:
        if node.get("type") == "entryPoint":
            continue  # Entry point doesn't need a function

        function_name = _get_function_name(node)
        node_data = node.get("data", {})

        code_lines.append(f"    def {function_name}(self, state: AgentState) -> AgentState:")
        code_lines.append('        """')
        code_lines.append(f"        {node_data.get('description') or node_data.get('label') or 'Node function'}")
        code_lines.append(f"        Type: {node.get('type')}")
        code_lines.append('        """')

        if node.get("type") == "process":
            # Process node with LLM
            model = node_data.get("model", "gpt-4o-mini")
            temperature = node_data.get("temperature", 0.7)
            max_tokens = node_data.get("maxTokens", 1000)
            model_class = _get_model_class(model)

            # Extract system prompt from node description/label
            system_prompt = node_data.get("description") or node_data.get("label") or "Process this task"

            code_lines.extend(
                [
                    "        ",
                    f"        # Initialize {model}",
                    f"        llm = {model_class}(",
                    f'            model="{model}",',
                    f"            temperature={temperature},",
                    f"            max_tokens={max_tokens},",
                    "        )",
                    "        ",
                    "        # System prompt guides the LLM behavior for this specific task",
                    f'        system_prompt = """{system_prompt}"""',
                    "        ",
                    "        # Process messages with LLM",
                    '        messages = state["messages"]',
                    "        if not messages:",
                    '            messages = [HumanMessage(content="Start workflow")]',
                    "        ",
                    "        # Prepend SystemMessage with task instructions",
                    "        messages = [SystemMessage(content=system_prompt)] + list(messages)",
                    "        ",
                    "        response = llm.invoke(messages)",
                    "        ",
                ]
            )

        elif node.get("type") == "toolCall":
            # Tool call node
            tool_name = node_data.get("tool", "unknown_tool")
            code_lines.extend(
                [
                    "        ",
                    f"        # Call tool: {tool_name}",
                    "        # TODO: Implement tool call logic",
                    "        # tools = [...]  # Define your tools",
                    "        # tool_node = ToolNode(tools)",
                    "        # result = tool_node.invoke(state)",
                    "        ",
                ]
            )

        elif node.get("type") == "human":
            # Human input node - pauses execution and waits for user input
            prompt = node_data.get("prompt", "Please provide input")
            timeout = node_data.get("timeout", 3600)
            node_id = node.get("id", "unknown")

            code_lines.extend(
                [
                    "        ",
                    "        # Human input required - pause execution",
                    "        # This will interrupt the graph and wait for user input",
                    "        user_input = interrupt({",
                    '            "type": "human_input",',
                    f'            "prompt": "{prompt}",',
                    f'            "node_id": "{node_id}",',
                    f'            "timeout": {timeout}',
                    "        })",
                    "        ",
                    "        # Add user input to message history",
                    '        updated_messages = list(state.get("messages", []))',
                    "        updated_messages.append(HumanMessage(content=str(user_input)))",
                    "        ",
                    "        # Return updated state",
                    "        return {",
                    '            "messages": updated_messages',
                    "        }",
                ]
            )
            # Skip standard return statement generation for human nodes
            code_lines.append("")
            code_lines.append("")
            continue

        elif node.get("type") == "decision":
            # Decision node with conditions
            conditions = node_data.get("conditions", [])
            code_lines.extend(
                [
                    "        ",
                    "        # Decision logic with conditions",
                ]
            )

            if conditions:
                for i, condition in enumerate(conditions):
                    field = condition.get("field", "field")
                    operator = condition.get("operator", "==")
                    value = condition.get("value", "")
                    target = condition.get("target", "END")

                    if i == 0:
                        code_lines.append(f'        if state.get("{field}") {operator} "{value}":')
                    else:
                        code_lines.append(f'        elif state.get("{field}") {operator} "{value}":')
                    code_lines.append(f'            return {{"next_node": "{target}"}}')

                code_lines.extend(
                    [
                        "        else:",
                        "            # Default path",
                    ]
                )
            else:
                code_lines.append("        # No conditions defined, using default path")

            code_lines.append("        ")

        elif node.get("type") == "deepPlanner":
            # Deep Planner node
            strategy = node_data.get("strategy", "hierarchical")
            max_subtasks = node_data.get("max_subtasks", 10)
            code_lines.extend(
                [
                    "        ",
                    "        # Deep Planner: Task decomposition",
                    f"        # Strategy: {strategy}",
                    f"        # Max subtasks: {max_subtasks}",
                    "        from middleware.deep_agent_middleware import TodoListMiddleware",
                    "        ",
                    "        todo_middleware = TodoListMiddleware()",
                    "        ",
                    "        # TODO: Implement task decomposition logic",
                    f"        # Example: Break down the current task into {max_subtasks} subtasks",
                    "        tasks = []  # Define your subtasks here",
                    "        todo_middleware.write_todos(tasks)",
                    "        ",
                ]
            )

        elif node.get("type") == "deepExecutor":
            # Deep Executor node
            model = node_data.get("model", "claude-sonnet-4-5-20250929")
            enable_todos = node_data.get("enable_todos", True)
            enable_filesystem = node_data.get("enable_filesystem", True)
            enable_subagents = node_data.get("enable_subagents", False)
            enable_summarization = node_data.get("enable_summarization", True)
            max_iterations = node_data.get("max_iterations", 5)
            model_class = _get_model_class(model)

            code_lines.extend(
                [
                    "        ",
                    "        # Deep Executor: Execute tasks with middleware",
                    "        from middleware.deep_agent_middleware import create_deep_agent_config",
                    "        ",
                    "        # Configure deep agent",
                    "        config = create_deep_agent_config(",
                    f'            model="{model}",',
                    f"            enable_todos={enable_todos},",
                    f"            enable_filesystem={enable_filesystem},",
                    f"            enable_subagents={enable_subagents},",
                    f"            enable_summarization={enable_summarization},",
                    "        )",
                    "        ",
                    "        # Initialize LLM",
                    f'        llm = {model_class}(model="{model}")',
                    "        ",
                    "        # TODO: Execute current task with middleware support",
                    f"        # Max iterations: {max_iterations}",
                    '        messages = state.get("messages", [])',
                    "        response = llm.invoke(messages)",
                    "        ",
                ]
            )

        elif node.get("type") == "deepCritic":
            # Deep Critic node
            validation_criteria = node_data.get("validation_criteria", ["completeness", "accuracy", "clarity"])
            quality_threshold = node_data.get("quality_threshold", 0.8)
            auto_replan = node_data.get("auto_replan", True)

            code_lines.extend(
                [
                    "        ",
                    "        # Deep Critic: Quality validation",
                    f"        # Validation criteria: {', '.join(validation_criteria)}",
                    f"        # Quality threshold: {quality_threshold}",
                    f"        # Auto-replan: {auto_replan}",
                    "        ",
                    "        # TODO: Implement quality validation logic",
                    "        # Evaluate output against criteria",
                    "        quality_score = 1.0  # Calculate quality score",
                    "        ",
                    f"        if quality_score < {quality_threshold}:",
                    f'            state["should_replan"] = {auto_replan}',
                    '            state["critic_feedback"] = "Quality below threshold"',
                    "        ",
                ]
            )

        elif node.get("type") == "subAgent":
            # Sub-Agent node
            specialization = node_data.get("specialization", "Research")
            max_depth = node_data.get("max_depth", 3)
            tools = node_data.get("tools", [])

            code_lines.extend(
                [
                    "        ",
                    "        # Sub-Agent: Delegate to specialized agent",
                    f"        # Specialization: {specialization}",
                    f"        # Max depth: {max_depth}",
                    "        from middleware.deep_agent_middleware import SubAgentMiddleware",
                    "        ",
                    "        subagent_middleware = SubAgentMiddleware(state)",
                    "        ",
                    "        # TODO: Spawn sub-agent for specialized task",
                    "        result = subagent_middleware.spawn_subagent(",
                    f'            name="{specialization.lower()}_agent",',
                    f'            description="{specialization} specialist",',
                    f'            task="Perform {specialization} task",',
                    f"            tools={tools},",
                    "        )",
                    "        ",
                ]
            )

        elif node.get("type") == "contextManager":
            # Context Manager node
            size_threshold = node_data.get("size_threshold", 170000)
            offload_strategy = node_data.get("offload_strategy", "auto")
            auto_summarize = node_data.get("auto_summarize", True)

            code_lines.extend(
                [
                    "        ",
                    "        # Context Manager: Offload and summarize large outputs",
                    f"        # Size threshold: {size_threshold} chars",
                    f"        # Strategy: {offload_strategy}",
                    "        from middleware.deep_agent_middleware import SummarizationMiddleware",
                    "        ",
                    "        summarization_middleware = SummarizationMiddleware(",
                    f"            size_threshold={size_threshold}",
                    "        )",
                    "        ",
                    "        # TODO: Manage context and summarize if needed",
                    "        # Check message history size and offload if necessary",
                    '        messages = state.get("messages", [])',
                    "        total_size = sum(len(str(m)) for m in messages)",
                    "        ",
                    f"        if total_size > {size_threshold}:",
                    "            # Summarize and offload",
                    "            summary = summarization_middleware.maybe_summarize(",
                    '                str(messages), "context_key"',
                    "            )",
                    "        ",
                ]
            )

        elif node.get("type") == "end":
            # End node
            code_lines.extend(
                [
                    "        ",
                    "        # Terminate workflow",
                    "        return state",
                ]
            )
            # Skip the next_node setting for end nodes
            code_lines.append("")
            code_lines.append("")
            continue

        # Add return statement that updates global state
        code_lines.extend(
            [
                "        # Update state with response",
                '        updated_messages = list(state.get("messages", []))',
                "        if response:",
                '            updated_messages.append(AIMessage(content=str(response.content if hasattr(response, "content") else response)))',
                "        ",
                "        return {",
                '            "messages": updated_messages,',
                '            "final_response": str(response.content if hasattr(response, "content") else response) if response else ""',
                "        }",
                "    ",
                "    ",
            ]
        )

    # Graph construction is handled by agent_loader
    # agent_loader reads the YAML manifest and builds the StateGraph
    # This agent class only provides the handler methods
    code_lines.extend(
        [
            "# ============================================",
            "# Graph Construction",
            "# ============================================",
            "# Graph construction is handled by agent_loader.py",
            "# The agent_loader reads the YAML manifest, imports this class,",
            "# and builds the StateGraph with the global AgentState schema.",
            "",
        ]
    )

    return "\n".join(code_lines)


def _get_function_name(node: dict[str, Any]) -> str:
    """Convert node to valid Python function name using consistent pattern: process_<sanitized_node_id>"""
    node_id = node.get("id", "")

    # Sanitize node ID by replacing hyphens with underscores
    sanitized_id = node_id.replace("-", "_")

    # Remove any remaining non-alphanumeric characters (except underscores)
    sanitized_id = re.sub(r"[^a-z0-9_]", "_", sanitized_id.lower())

    # Ensure it doesn't start with a digit
    if sanitized_id and sanitized_id[0].isdigit():
        sanitized_id = f"node_{sanitized_id}"

    # Use consistent pattern: process_<node_id>
    return f"process_{sanitized_id}" if sanitized_id else "unnamed_node"


def _get_model_class(model: str) -> str:
    """Get the appropriate LangChain model class"""
    if not model:
        return "ChatOpenAI"

    if "claude" in model.lower():
        return "ChatAnthropic"
    elif "gpt" in model.lower() or "openai" in model.lower():
        return "ChatOpenAI"

    return "ChatOpenAI"


def _get_next_node(node_id: str, edges: list[dict[str, Any]]) -> str | None:
    """Find the next node ID from edges"""
    for edge in edges:
        if edge.get("source") == node_id:
            return edge.get("target")
    return None


def validate_graph(nodes: list[dict[str, Any]], edges: list[dict[str, Any]]) -> dict[str, Any]:
    """
    Validate graph structure before conversion.

    Returns:
        dict: Validation result with 'valid' bool and 'errors' list
    """
    errors = []

    # Check for entry point
    entry_nodes = [n for n in nodes if n.get("type") == "entryPoint"]
    if not entry_nodes:
        errors.append("No entry point node found")
    elif len(entry_nodes) > 1:
        errors.append("Multiple entry points found (only one allowed)")

    # Check for disconnected nodes
    node_ids = {n.get("id") for n in nodes}
    connected_nodes = set()

    if entry_nodes:
        # Start from entry point
        entry_id = entry_nodes[0].get("id")
        connected_nodes.add(entry_id)

        # BFS to find all connected nodes
        queue = [entry_id]
        while queue:
            current = queue.pop(0)
            for edge in edges:
                if edge.get("source") == current:
                    target = edge.get("target")
                    if target not in connected_nodes:
                        connected_nodes.add(target)
                        queue.append(target)

        # Check for disconnected nodes
        disconnected = node_ids - connected_nodes
        if disconnected:
            errors.append(f"Disconnected nodes: {', '.join(disconnected)}")

    # Check for invalid edges
    for edge in edges:
        if edge.get("source") not in node_ids:
            errors.append(f"Edge references non-existent source node: {edge.get('source')}")
        if edge.get("target") not in node_ids:
            errors.append(f"Edge references non-existent target node: {edge.get('target')}")

    # Check for cycles (simple check)
    # TODO: More sophisticated cycle detection if needed

    return {"valid": len(errors) == 0, "errors": errors}
